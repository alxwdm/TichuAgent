{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainAgents.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzHt6PahLWQT7YEiG1NnvH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fjkm2DIq0Tr",
        "colab_type": "text"
      },
      "source": [
        "# TichuAgents: Training the Agents\n",
        "\n",
        "This notebook is used for training Agents using Reinforcement Learning to play Tichu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrOs1k1rrC_I",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf0KUdu0qw2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute after restarting runtime\n",
        "!git clone https://github.com/alxwdm/tichuagent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf7HZOaZrFWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute when content on github changed\n",
        "%cd /content/tichuagent\n",
        "!git pull\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHWZ41PPrF1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('/content/tichuagent')\n",
        "# import Environment\n",
        "from env.env import Env\n",
        "# import all Agents\n",
        "from agents.heuristic.greedy import greedyAgent\n",
        "from agents.ddpg.ddpg_agent import DDPGAgent\n",
        "# import utility functions\n",
        "from utils import play_dumb_game, play_greedy_game"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEkCOuERsE67",
        "colab_type": "text"
      },
      "source": [
        "# Shared utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfjJAP0SsGTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def battle(agents, n_games=1000):\n",
        "    \"\"\" Lets trained agents play games against each other. \"\"\"\n",
        "    pass # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TbB0VCcrS1P",
        "colab_type": "text"
      },
      "source": [
        "# DDPG Agent\n",
        "\n",
        "Deep Deterministic Policy Gradient is an off-policy RL approach that combines both value- and policy-based learning. The DDPG actor learns how to act (i.e. policy-based), and a critic learns how to estimate the current situation (i.e. value-based). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI9cMP4nr87n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('/content/tichuagent')\n",
        "from itertools import compress\n",
        "# import Environment\n",
        "from env.env import Env\n",
        "from env.cards import Cards\n",
        "from env.deck import Deck\n",
        "# import all Agents\n",
        "from agents.heuristic.greedy import greedyAgent\n",
        "from agents.ddpg.ddpg_agent import DDPGAgent\n",
        "# import utility functions\n",
        "from utils import play_dumb_game, play_greedy_game\n",
        "\n",
        "def _vec_to_cards(vec):\n",
        "    \"\"\" Turns a vector representation into a Cards instance. \"\"\"\n",
        "    all_cards = Deck().all_cards\n",
        "    return Cards(list(compress(all_cards, vec)))\n",
        "\n",
        "def ddpg(n_episodes=100, episode_offset=0, checkpoint_path=None,\n",
        "         eps_start = 0.1, eps_decay = 0.995):\n",
        "    \"\"\" Trains a DDPG Agent on Tichu. \"\"\"\n",
        "    # initialize environment and agent\n",
        "    env = Env()\n",
        "    state_size, action_size = env.info()\n",
        "    heuristic_agent = greedyAgent()\n",
        "    agent = DDPGAgent(state_size=state_size, action_size=action_size,\n",
        "                  random_seed=0, heuristic_agent=heuristic_agent)\n",
        "    all_scores = []\n",
        "    eps = eps_start\n",
        "    # reload checkpoint from previous training if available\n",
        "    if checkpoint_path:\n",
        "        agent.load_checkpoint(filepath=checkpoint_path)\n",
        "    # train for n_episodes\n",
        "    for i_episode in range(episode_offset, n_episodes + episode_offset):\n",
        "        state, reward, done, active_player = env.reset()\n",
        "        action_buffer = [None, None, None, None]\n",
        "        scores = [0, 0, 0, 0]\n",
        "        nstep = 0\n",
        "        init_cnt = 0\n",
        "        invalid_init = 0\n",
        "        # make a valid initial move from heuristic agent (first steps)\n",
        "        # each player must make an initial move before learning,\n",
        "        # because of reward-design (reward is valid every 4 steps)\n",
        "        print(\"\\n\")\n",
        "        for i in range(4):\n",
        "            last_active = active_player\n",
        "            action_buffer[active_player] = agent.act(state[active_player],\n",
        "                                                         1) # 1: heuristic move\n",
        "            state, reward, done, active_player = env.step(active_player,\n",
        "                                                  action_buffer[active_player])\n",
        "            print(last_active, active_player, np.shape(action_buffer[last_active]))\n",
        "        # train one episode\n",
        "        while True:\n",
        "            prev_state = state\n",
        "            # regular learning routine after initialization:\n",
        "            # learn from previous step, then take next step\n",
        "            # vice-versa not possible (because of state/reward validity)\n",
        "            try:\n",
        "                assert np.shape(action_buffer[active_player]) == (56,), \\\n",
        "                    \"nstep {}, Shape of action buffer must be (56,1) but it is {}\".format(\n",
        "                        nstep, np.shape(action_buffer[active_player]))\n",
        "            except AssertionError:\n",
        "                print('assertion failed, showing last actions')\n",
        "                for i in range(4):\n",
        "                    if action_buffer[i] is not None:\n",
        "                        cards = _vec_to_cards(action_buffer[i])\n",
        "                        cards.show()\n",
        "                    else:\n",
        "                        print('Player {} NoneType action_buffer!'.format(i))\n",
        "            agent.step(prev_state[active_player],\n",
        "                       action_buffer[active_player],\n",
        "                       reward[active_player],\n",
        "                       state[active_player],\n",
        "                       done, nstep)\n",
        "            # add rewards to scores list\n",
        "            scores[active_player] += reward[active_player]\n",
        "            # take an action in the environment\n",
        "            prev_state[active_player] = state[active_player]\n",
        "            action_buffer[active_player] = agent.act(state[active_player], eps)\n",
        "            state, reward, done, active_player = env.step(active_player,\n",
        "                                                  action_buffer[active_player])\n",
        "            nstep += 1\n",
        "            # all agents take a step when game is finished\n",
        "            if done:\n",
        "                for i in range(4):\n",
        "                    assert np.shape(action_buffer[active_player]) == (56,), \\\n",
        "                    \"Shape of action buffer must be (56,1) but it is {}\".format(\n",
        "                        np.shape(action_buffer[active_player]))\n",
        "                    agent.step(prev_state[i], action_buffer[i],\n",
        "                               reward[i], state[i], done, nstep)\n",
        "                break\n",
        "        # print episode info\n",
        "        print('\\rEpisode: {} \\t Steps: {} \\t Avg score: {}'.format(i_episode,\n",
        "                                                    nstep, np.mean(scores)),\n",
        "              end='')\n",
        "        if i_episode > 0 and i_episode%10 == 0:\n",
        "            print('\\nEpisode: {} \\t Steps: {} \\t Avg score: {}'.format(i_episode,\n",
        "                                                    nstep, np.mean(scores)))\n",
        "        # take average rewards of all agents\n",
        "        all_scores.append(np.mean(scores))\n",
        "        eps = eps_decay * eps # decrease epsilon\n",
        "    # save checkpoints\n",
        "    fpath = 'checkpoint_' + str(i_episode)\n",
        "    agent.save_checkpoint(filename=fpath)\n",
        "    return all_scores"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzo9V5JMh7yv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e0a0ff8-901d-49a8-85ac-7de50e28346b"
      },
      "source": [
        "all_scores = ddpg(eps_decay=1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "1 2 (56,)\n",
            "2 3 (56,)\n",
            "Episode: 0 \t Steps: 752 \t Avg score: -1683.75\n",
            "\n",
            "2 3 (56,)\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "1 2 (56,)\n",
            "Episode: 1 \t Steps: 755 \t Avg score: -1690.0\n",
            "\n",
            "1 2 (56,)\n",
            "2 3 (56,)\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "Episode: 2 \t Steps: 833 \t Avg score: -1911.25\n",
            "\n",
            "1 2 (56,)\n",
            "2 3 (56,)\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "Episode: 3 \t Steps: 1015 \t Avg score: -2302.5\n",
            "\n",
            "0 1 (56,)\n",
            "1 2 (56,)\n",
            "2 3 (56,)\n",
            "3 0 (56,)\n",
            "Episode: 4 \t Steps: 779 \t Avg score: -1746.25\n",
            "\n",
            "1 2 (56,)\n",
            "2 3 (56,)\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "Episode: 5 \t Steps: 550 \t Avg score: -1270.0\n",
            "\n",
            "1 2 (56,)\n",
            "2 3 (56,)\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "Episode: 6 \t Steps: 521 \t Avg score: -1158.75\n",
            "\n",
            "1 3 (56,)\n",
            "3 0 (56,)\n",
            "0 1 (56,)\n",
            "1 2 (56,)\n",
            "assertion failed, showing last actions\n",
            "┍┄┄┄┑┍┄┄┄┑┍┄┄┄┑┍┄┄┄┑┍┄┄┄┑\n",
            "┆ 7   ┆┆ 7   ┆┆ Q   ┆┆ Q   ┆┆ Q   ┆\n",
            "┆  ♠  ┆┆  ♢  ┆┆  ♠  ┆┆  ♡  ┆┆  ♢  ┆\n",
            "┆   7 ┆┆   7 ┆┆   Q ┆┆   Q ┆┆   Q ┆\n",
            "┖┄┄┄┚┖┄┄┄┚┖┄┄┄┚┖┄┄┄┚┖┄┄┄┚\n",
            "  PASS\n",
            "Player 2 NoneType action_buffer!\n",
            "┍┄┄┄┑┍┄┄┄┑┍┄┄┄┑┍┄┄┄┑┍┄┄┄┑\n",
            "┆ Ph  ┆┆ 3   ┆┆ 3   ┆┆ 9   ┆┆ 9   ┆\n",
            "┆ oe  ┆┆  ♠  ┆┆  ♡  ┆┆  ♠  ┆┆  ♢  ┆\n",
            "┆ nix ┆┆   3 ┆┆   3 ┆┆   9 ┆┆   9 ┆\n",
            "┖┄┄┄┚┖┄┄┄┚┖┄┄┄┚┖┄┄┄┚┖┄┄┄┚\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-923f805b88e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-656636fe4a19>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, episode_offset, checkpoint_path, eps_start, eps_decay)\u001b[0m\n\u001b[1;32m     72\u001b[0m                        \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                        \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                        done, nstep)\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;31m# add rewards to scores list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tichuagent/agents/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, timestep)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mLEARN_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEARN_STEP_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tichuagent/agents/utils/replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m           experiences if e is not None])).float().to(device)\n\u001b[1;32m     46\u001b[0m         actions = torch.from_numpy(np.vstack([e.action for e in\n\u001b[0;32m---> 47\u001b[0;31m           experiences if e is not None])).long().to(device)\n\u001b[0m\u001b[1;32m     48\u001b[0m         rewards = torch.from_numpy(np.vstack([e.reward for e in\n\u001b[1;32m     49\u001b[0m           experiences if e is not None])).float().to(device)\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 56 and the array at index 125 has size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhRkP7LdiHBE",
        "colab_type": "text"
      },
      "source": [
        "# Debugging Area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd2jfwhDh_nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nothing here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}