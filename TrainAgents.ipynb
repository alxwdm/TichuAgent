{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainAgents.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNas14kL9WepsSOH/2DhD0l"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fjkm2DIq0Tr",
        "colab_type": "text"
      },
      "source": [
        "# TichuAgents: Training the Agents\n",
        "\n",
        "This notebook is used for training Agents using Reinforcement Learning to play Tichu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrOs1k1rrC_I",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf0KUdu0qw2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute after restarting runtime\n",
        "!git clone https://github.com/alxwdm/tichuagent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf7HZOaZrFWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute when content on github changed\n",
        "%cd /content/tichuagent\n",
        "!git pull\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHWZ41PPrF1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('/content/tichuagent')\n",
        "# import Environment\n",
        "from env.env import Env\n",
        "# import all Agents\n",
        "from agents.heuristic.greedy import greedyAgent\n",
        "from agents.ddpg.ddpg_agent import DDPGAgent\n",
        "# import utility functions\n",
        "from utils import play_dumb_game, play_greedy_game"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEkCOuERsE67",
        "colab_type": "text"
      },
      "source": [
        "# Shared utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfjJAP0SsGTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def battle(agents, n_games=1000):\n",
        "    \"\"\" Lets trained agents play games against each other. \"\"\"\n",
        "    pass # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TbB0VCcrS1P",
        "colab_type": "text"
      },
      "source": [
        "# DDPG Agent\n",
        "\n",
        "Deep Deterministic Policy Gradient is an off-policy RL approach that combines both value- and policy-based learning. The DDPG actor learns how to act (i.e. policy-based), and a critic learns how to estimate the current situation (i.e. value-based). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI9cMP4nr87n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('/content/tichuagent')\n",
        "# import Environment\n",
        "from env.env import Env\n",
        "# import all Agents\n",
        "from agents.heuristic.greedy import greedyAgent\n",
        "from agents.ddpg.ddpg_agent import DDPGAgent\n",
        "# import utility functions\n",
        "from utils import play_dumb_game, play_greedy_game\n",
        "\n",
        "PRINT_EVERY = 20\n",
        "\n",
        "def ddpg(n_episodes=100, episode_offset=0, checkpoint_path=None,\n",
        "         eps_start = 0.3, eps_decay = 0.995, max_steps = 1000):\n",
        "    \"\"\" Trains a DDPG Agent on Tichu. \"\"\"\n",
        "    # initialize environment and agent\n",
        "    env = Env(train_mode=True)\n",
        "    state_size, action_size = env.info()\n",
        "    heuristic_agent = greedyAgent()\n",
        "    agent = DDPGAgent(state_size=39, action_size=17,\n",
        "                  random_seed=8765, heuristic_agent=heuristic_agent)\n",
        "    all_scores = []\n",
        "    all_steps = []\n",
        "    all_valid_moves = []\n",
        "    eps = eps_start\n",
        "    # reload checkpoint from previous training if available\n",
        "    if checkpoint_path:\n",
        "        agent.load_checkpoint(filepath=checkpoint_path)\n",
        "    # train for n_episodes\n",
        "    for i_episode in range(episode_offset, n_episodes + episode_offset):\n",
        "        state, reward, done, active_player = env.reset()\n",
        "        action_buffer = [None, None, None, None]\n",
        "        scores = [0, 0, 0, 0]\n",
        "        nstep = 0\n",
        "        init_cnt = 0\n",
        "        agent_move_cnt = 0\n",
        "        eps_move_cnt = 0\n",
        "        # make a valid initial move from heuristic agent (first steps)\n",
        "        # each player must make an initial move before learning,\n",
        "        # because of reward-design (reward is valid every 4 steps)\n",
        "        idle_cnt = 0\n",
        "        while any(elem is None for elem in action_buffer):\n",
        "            action_buffer[active_player], eps_move = agent.act(\n",
        "                                                    state[active_player], 1)\n",
        "            state, reward, done, active_player = env.step(active_player,\n",
        "                                            action_buffer[active_player])            \n",
        "            idle_cnt += 1\n",
        "            if idle_cnt > 10:\n",
        "                raise EnvironmentError(\"Something went wrong.\")\n",
        "                #return state[active_player]\n",
        "        # train one episode\n",
        "        while nstep < max_steps:\n",
        "            prev_state = state\n",
        "            prev_active = active_player\n",
        "            # regular learning routine after initialization:\n",
        "            # learn from previous step, then take next step\n",
        "            # vice-versa not possible (because of state/reward validity)\n",
        "            agent.step(prev_state[active_player],\n",
        "                       action_buffer[active_player],\n",
        "                       reward[active_player],\n",
        "                       state[active_player],\n",
        "                       done, nstep)\n",
        "            # add rewards to scores list\n",
        "            scores[active_player] += reward[active_player]\n",
        "            # take an action in the environment\n",
        "            prev_state[active_player] = state[active_player]\n",
        "            action_buffer[active_player], eps_move = agent.act(\n",
        "                                                     state[active_player], eps)\n",
        "            state, reward, done, active_player = env.step(active_player,\n",
        "                                                  action_buffer[active_player])\n",
        "            nstep += 1\n",
        "            # count successfull ddpg moves\n",
        "            if not(prev_active == active_player) and not(eps_move):\n",
        "                agent_move_cnt += 1\n",
        "            elif not(prev_active == active_player) and eps_move:\n",
        "                eps_move_cnt += 1\n",
        "            # all agents take a step when game is finished\n",
        "            if done:\n",
        "                for i in range(4):\n",
        "                    agent.step(prev_state[i], action_buffer[i],\n",
        "                               reward[i], state[i], done, nstep)\n",
        "                break\n",
        "        # print episode info\n",
        "        print(('\\rEpisode: {} \\t Total Steps: {} \\t Valid Agent Steps: {} \\t' +\n",
        "               'Valid eps steps: {} \\t Avg score: {} \\t Current eps: {}').format(\n",
        "            i_episode, nstep, agent_move_cnt, eps_move_cnt, np.mean(scores), eps),\n",
        "              end='')\n",
        "        if i_episode > 0 and i_episode % PRINT_EVERY == 0:\n",
        "            print('')\n",
        "        # take average statistics of all agents\n",
        "        all_scores.append(np.mean(scores))\n",
        "        all_steps.append(nstep)\n",
        "        all_valid_moves.append(agent_move_cnt)\n",
        "        eps = eps_decay * eps # decrease epsilon\n",
        "    # save checkpoints\n",
        "    fpath = 'checkpoint_' + str(i_episode)\n",
        "    agent.save_checkpoint(filename=fpath)\n",
        "    return all_scores, all_steps, all_valid_moves"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzo9V5JMh7yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_scores, all_steps, all_valid_moves = ddpg(n_episodes=1000, eps_decay=0.997)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhRkP7LdiHBE",
        "colab_type": "text"
      },
      "source": [
        "# Debugging Area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctv8uClK0Agh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import compress\n",
        "\n",
        "from env.cards import Cards\n",
        "from env.deck import Deck\n",
        "from env.player import Player"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd2jfwhDh_nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for debugging, use this function to nicely print action vector as cards:\n",
        "# example: _vec_to_cards(action).show()\n",
        "def _vec_to_cards(vec):\n",
        "    \"\"\" Turns a vector representation into a Cards instance. \"\"\"\n",
        "    all_cards = Deck().all_cards\n",
        "    return Cards(list(compress(all_cards, vec)))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKQL4RlbwJW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _flatten_state(state):\n",
        "    \"\"\" A very ugly state flattening function. TODO! \"\"\"\n",
        "    flattened_list = [item for sublist in state for item in sublist]\n",
        "    flattened_state = []\n",
        "    for elem in flattened_list:\n",
        "        if type(elem) != list:\n",
        "            flattened_state.append(elem)\n",
        "        else:\n",
        "            for e in elem:\n",
        "                flattened_state.append(e)\n",
        "    return np.asarray(flattened_state, dtype='int32')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCj9-6EuvxmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try alternative state/action design\n",
        "env = Env()\n",
        "state, reward, done, active_player = env.reset()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxenDIIcwN4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ebce9bf7-7cd9-417a-9bdf-9c8db222034c"
      },
      "source": [
        "def state_conv_suitless(state_vec):\n",
        "    \"\"\"\n",
        "    An alternative state with reduced state space size.\n",
        "    \n",
        "    state design:\n",
        "    [hand_size, tichu_flag, suitless_cards] of active player\n",
        "    [is_opponent, hand_size, tichu_flag, suitless_cards] of stack leader\n",
        "\n",
        "    returns state vector in flattened format\n",
        "    \"\"\"\n",
        "    def suitless_enc(crd_state):\n",
        "        suitless_cards = np.zeros(17, int).tolist()\n",
        "        for i in range(13):\n",
        "            suitless_cards[i] = sum(crd_state[i*4:i*4+4])\n",
        "        suitless_cards[13] = crd_state[13]\n",
        "        suitless_cards[14] = crd_state[14]\n",
        "        suitless_cards[15] = crd_state[15]\n",
        "        suitless_cards[16] = crd_state[16]\n",
        "        return suitless_cards\n",
        "\n",
        "    def _flatten_conv_state(state):\n",
        "        \"\"\" A state flattening function for suitless state. \"\"\"\n",
        "        flattened_list = state\n",
        "        flattened_state = []\n",
        "        for elem in flattened_list:\n",
        "            if type(elem) != list:\n",
        "                flattened_state.append(elem)\n",
        "            else:\n",
        "                for e in elem:\n",
        "                    flattened_state.append(e)\n",
        "        return np.asarray(flattened_state, dtype='int32')\n",
        "\n",
        "    # get info from full state\n",
        "    hand_size = state_vec[0][0][0]\n",
        "    hand_cards = _vec_to_cards(state_vec[0][0][2])\n",
        "    opp_cards_0 = _vec_to_cards(state_vec[0][1][2])\n",
        "    teammate_cards = _vec_to_cards(state_vec[0][2][2])\n",
        "    opp_cards_1 = _vec_to_cards(state_vec[0][3][2])\n",
        "    # determine leading cards\n",
        "    # new stack\n",
        "    if (teammate_cards.type == 'pass' and opp_cards_0.type == 'pass' and \n",
        "            opp_cards_1.type == 'pass'):\n",
        "        leading_idx = 0\n",
        "        is_opponent = 0\n",
        "        leading_cards = Cards([])\n",
        "    # teammate leading\n",
        "    elif teammate_cards.power > max(opp_cards_0.power, opp_cards_1.power):\n",
        "        leading_idx = 2\n",
        "        is_opponent = 0\n",
        "        leading_cards = teammate_cards\n",
        "    # opponent 0 leading\n",
        "    elif ((opp_cards_0.power > opp_cards_1.power) or \n",
        "            (opp_cards_1.type == 'pass')):\n",
        "        leading_idx = 1\n",
        "        is_opponent = 1\n",
        "        leading_cards = opp_cards_0\n",
        "    # opponent 1 leading\n",
        "    else:\n",
        "        leading_idx = 3\n",
        "        is_opponent = 1\n",
        "        leading_cards = opp_cards_1\n",
        "    # get first part of state: self perspective\n",
        "    conv_state = []\n",
        "    conv_state.append(state[0][0][0]) # Hand Size\n",
        "    conv_state.append(state[0][0][1]) # Tichu Flag\n",
        "    conv_state.append(suitless_enc(state[0][0][2])) # suitless encoded hand\n",
        "    # get second part of state: leading player perspective\n",
        "    if leading_idx == 0:\n",
        "        leading_size = 0\n",
        "        leading_tichu = 0\n",
        "        leading_suitless = np.zeros(17, int).tolist()\n",
        "    else:\n",
        "        leading_size = leading_cards.size\n",
        "        leading_tichu = state[0][leading_idx][1]\n",
        "        leading_suitless = suitless_enc(state[0][leading_idx][2])\n",
        "    conv_state.append(is_opponent) # opponent yes/no\n",
        "    conv_state.append(leading_size) # hand size\n",
        "    conv_state.append(leading_tichu) # tichu flag\n",
        "    conv_state.append(leading_suitless) # suitless encoded leading cards\n",
        "\n",
        "    print(conv_state[0:3])\n",
        "    print(conv_state[3:])\n",
        "    return _flatten_conv_state(conv_state)\n",
        "\n",
        "_ = state_conv_suitless(state)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14, 0, [2, 0, 3, 1, 0, 2, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0]]\n",
            "[0, 0, 0, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drDCu1ir4qeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def action_conv_suitless(suitless_action, state_cards):\n",
        "    \"\"\"\n",
        "    Converts a \"suitless\" action into action vector expected by env.\n",
        "    \n",
        "    Example:\n",
        "    suitless_action: [2, 2, 0, 0, ...] for a 2-3-pair sequence\n",
        "    action_vector: [1, 1, 0, 0, 1, 1, 0, 0, ...] depending on available cards\n",
        "    \"\"\"\n",
        "    action_vec = np.zeros(len(56), int)\n",
        "    # encode regular cards:\n",
        "    for i in range(13):\n",
        "        card_count = suitless_action[i]\n",
        "        available_cards = state_cards[i*4:i*4+4]\n",
        "        if card_count == 0:\n",
        "            pass\n",
        "        elif sum(available_cards) < card_count:\n",
        "            suc = False\n",
        "            break\n",
        "        else:\n",
        "            for j in range(4):\n",
        "                if available_cards[j] == 1 and card_count > 0:\n",
        "                    action_vec[i*4+j] = 1\n",
        "                    card_count -= 1\n",
        "                else:\n",
        "                    pass\n",
        "    # encode special cards\n",
        "    action_vec[-4] = suitless_action[13]\n",
        "    action_vec[-3] = suitless_action[14]\n",
        "    action_vec[-2] = suitless_action[15]\n",
        "    action_vec[-1] = suitless_action[16]\n",
        "\n",
        "    return action_vec"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}